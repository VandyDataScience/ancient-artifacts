---
title: "40-modeling"
output:
  pdf_document: default
  html_document: default
---

This model will end up containing some generalized method that we'll use for all the models.  This will aid us in maintainability of our codebase, and also help us avoid copy/paste (and lack thereof) replication errors.

**Note that this notebook requires 10 to already have been run.**

```{r modeling imports}
pacman::p_load(tidymodels, usemodels)
```

# Split the data
In this step, we perform a general data split as normal.  We'll split with 75% to training and the other 25% to testing, stratified by the class.  Note that we want to abstain from looking at the performance on the test data until the _very, VERY_ end.  We might also want to think about unsetting this seed once the code is developed, and just recording whatever seed is randomly generated.  This will allow us some variation on our generated data.

```{r ml data split}
set.seed(2434)
data_split <- initial_split(artifact_data, prop=3/4, strata=particle_class)

train_data <- training(data_split)
test_data <- testing(data_split)
```


# Cross-validation splits
Here, I'll add the cross validation splits for tuning.  We'll use 5 fold cross-validation here, and we'll stratify by particle class one again.
```{r xval data split}
cv_folds <- vfold_cv(train_data, v=5, strata=particle_class)
cv_folds
```


```{r generalized function for prediction data frames}
get_prediction_dataframes <- function(final_fit, train_data) {
#get prediction class and probabilities
hp_training_preds <- 
  predict(final_fit, train_data) %>%
  bind_cols(predict(final_fit, train_data, type = "prob")) %>% 
  bind_cols(train_data %>% 
              select(particle_class))

return (hp_training_preds)
}
```

```{r calculate confusion matrix and produce summary and confusion matrix}
calculate_confusion_matrix <-function (hp_training_preds) {
#calculate confusion matrix
train_conf <- hp_training_preds %>%
  conf_mat(particle_class, .pred_class) 

#get summary info
t1 <- train_conf %>%
  summary() %>%
  select(-.estimator) %>%
  gridExtra::tableGrob(rows=NULL, theme=gridExtra::ttheme_default(base_size=10))

#plot cmat info
cm <- train_conf %>%
  autoplot(type='heatmap') +
  labs(title='Confusion matrix for training data')

gridExtra::grid.arrange(cm, t1, ncol=2)

return (t1, cm)
}
```

```{r}
calculate_best_performance_metrics <- function (fold_metrics, best_params) {
  #get metrics
  best_fold_metrics <- fold_metrics %>%
  filter(.config==best_params$.config[[1]])

#plot
best_fold_metrics %>%
  mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
  ggplot(aes(x=.metric, y=.estimate, fill=.metric)) +
  geom_boxplot(outlier.shape = NA, na.rm=TRUE) +
  geom_jitter(aes(x=.metric, y=.estimate), na.rm=TRUE) +
  facet_grid(cols=vars(facet_val), scales='free') + #just to get on separate plots
  labs(title='Distribution of cross validation metrics for best hyperparameter set',
       subtitle='By metric',
       x='metric',
       y='metric estimate') +
  theme(legend.position = "none")

  return (best_fold_metrics)
}
```

