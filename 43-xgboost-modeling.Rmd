---
title: "43-xgboost-modeling"
output:
  html_notebook:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: true
---

In this notebook, we use xgboost to perform modeling.  Our general approach will be to use hyperparameter tuning via cross-validation to identify the best performing hyperparameters,  In another notebook, we will investigate the performance of the model.

### Useful packages
```{r xgb specific modeling packages, results='hide'}
#load previous notebook data
source(knitr::purl("40-modeling.Rmd"))
fs::file_delete("40-modeling.R")

pacman::p_load(glmnet, tictoc, vip, tidytext, xgboost, recipes, parsnip)
```

Here, we define the specs for the feature engineering, the model, the generalized workflow, and the parameters that we'll tune using parameters selected from a max entropy grid.  These were generally obtained from the `use_*` functions of the `usemodels` package shown above.
```{r xgb tidymodel specs}
xgb_recipe <- 
  recipe(formula = particle_class ~ ., data = train_data) %>% 
  update_role(id, img_id, starts_with('filter'), hash, new_role='no_model') %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors(), -all_nominal()) 
xgb_spec <- 
  boosted_tree_mod <-boost_tree(mode = "classification", mtry = tune(), min_n = tune()) %>% 
  set_engine("xgboost") 
xgb_workflow <- 
  workflow() %>% 
  add_recipe(xgb_recipe) %>% 
  add_model(xgb_spec) 
xgb_parameters <- parameters(xgb_spec) %>%
  update(mtry=mtry(c(1,ncol(train_data))))
xgb_grid <- grid_max_entropy(xgb_parameters, size=20)
xgb_grid
```

# Hyperparameter tuning (model selection) via cross-validation
```{r perform hyperparameter tuning via xvalidation, results='hide'}
tic()
xgb_tune <- xgb_workflow %>%
  tune_grid(resamples = cv_folds,
            grid = xgb_grid,
            metrics = metric_set(accuracy, roc_auc, pr_auc, sens, yardstick::spec, ppv, npv, f_meas),
            control = control_grid(verbose = TRUE, parallel_over='everything'))
toc()
```

## Cross-validation metric distributions
In this section, we're going to take a little bit of a look at the individualized performance of the models taking into each fold into account.  This will satisfy our academic curiosity in terms of machine learning and also provide some insight into the behaviors of the models.  We'll look more at the aggregated measures in a moment.

We'll first decompress the tuning metrics a bit to get them into a more friendly form for processing.  
```{r arrange cross validation metrics}
#extract the cross validation metrics for the xgb by fold (i.e., unsummarized)
xgb_fold_metrics <- xgb_tune %>%
  select(id, .metrics, .notes) %>%
  unnest(.metrics)
head(xgb_fold_metrics, 10)
```
Now, let's visualize this generalized performance over all the models, both the fantastic ones and the horrible ones.
```{r visualize fold performance distributions, fig.height=6, fig.width=12}
xgb_fold_metrics %>%
  mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
  ggplot(aes(x=.metric, y=.estimate, fill=id)) +
  geom_boxplot(outlier.colour = 'red', na.rm=TRUE) +
  facet_wrap(facet='.metric', scales='free', nrow=2) + 
  labs(title='Distribution of performance by 20 model candidates',
       subtitle='By fold and metric',
       x='metric',
       y='metric distribution') +
  scale_x_discrete(labels=NULL)
```
Across the majority of metrics, the folds appear to be generally consistent in distribution except for metrics ppv and spec where fold 5 is significantly lower in distribution.

# Identifying the "best" model
Now, let's collect the metrics to see how the model did over all of the folds and all of the metrics in order to identify the best model from these candidates.  Note that this tabe looks similar to the prior tibble; the main difference here is that the results are aggregated over the folds (hence the `mean` and `n` columns).

```{r aggregating hyperparameter tuning/cross validation metrics}
tune_metrics <- xgb_tune %>% 
  collect_metrics()
head(tune_metrics, 5)
```

## Basic performance overview
Let's just look at the overall (fold-less) distribution of the metrics.
```{r overall performance}
tune_metrics %>%
  ggplot(aes(x=.metric, y=mean)) +
  geom_boxplot(aes(fill=.metric), outlier.shape=NA, na.rm=TRUE) +
  geom_jitter(na.rm=TRUE) +
  facet_wrap(facets='.metric', nrow=2, scales='free') +
  theme(legend.position = 'none') +
  labs(title='Distribution of mean cv performance by 20 model candidates',
       subtitle='By metric',
       x='metric',
       y='mean cv metric') +
  scale_x_discrete(labels=NULL)
```
The general observations here are that for pr_auc, accuracy, roc_auc, and f_meas there are tighter distributions that are high performing while for ppv and spec there are larger distributions that are lower performance. Npv and sens are also high performing. pr_auc, accuracy, roc_auc, f_meas, npv, and sens are high performing with low performing outliers and ppv and spec have the opposite results: low performance with some high performing outliers.

## Making sense of the hyperparameters and their influence

Let's visualize this so we can make some sort of sense out of it.
```{r fig.height=6}
tune_metrics %>%
  mutate(.metric=fct_relevel(.metric, 'roc_auc', 'pr_auc', 'f_meas', 'accuracy', 'sens', 'spec')) %>%
  ggplot(aes(x=mtry, y=min_n)) +
  geom_point(aes(fill=mean), shape=22, size=6) + 
  scale_x_log10(guide=guide_axis(angle=45)) + 
  facet_wrap(ncol=4, facet='.metric') + 
  scale_fill_gradient2(low='red', mid='yellow', high='green', midpoint=0.5) + 
  labs(title='Mean performance of mixture/penalty hyperparameter combinations',
       subtitle='By performance metric',
         x='mtry',
         y='min_n',
         fill='mean cv value')
```
Here is a difficult-to-understand plot.  The objective of this visualization is to begin to digest the relationship between the two hyperparameters and the performance given a certain metric.  Recall that hyperparameter tuning evaluates all combinations of hyperparameters.  These combos are shown as a square on a particular "subplot" of a metric of interest.  Then, there are 20 squares since there are 20 models.  And, the arrangement of all the "intersection" squares is identical.

What is of interest here is the color of the squares.  Red indicates that the performance is poor, and green indicates that the performance is great. There seems to have little to no effect on the performance while increasing mtry, and this trend is generally the same for increasing min_n. For the sense metric there interestingly appears to be a slight effect when increasing mtry.

## Looking at all of the metrics together to select a model
One possible way of evaluating a good model might be to rank the model according to its performance across all of the metrics.  This allows us to get a bit away from the values themselves.  However, we can also look at the values themselves and investigate the relationship.
```{r overall performance assessment, fig.height=6}
#calculate mean metrics and rank
mdl_overall <- tune_metrics %>%
  group_by(.metric) %>%
  mutate(metr_rank=rank(-mean, ties.method='average')) %>% #-mean so that rank increases (so, worse) with decreasing metric
  group_by(.config, .add=FALSE) %>%
  mutate(mean_rank = mean(metr_rank)) %>% #add mean rank
  mutate(mean_value = -mean(mean, na.rm=TRUE)) %>% #add mean value
  pivot_longer(cols=c(mean_rank, mean_value), names_to = 'agg_perf_type', values_to='agg_perf') %>%
  group_by(agg_perf_type) %>%
  filter(.metric=='pr_auc') #just pick one set of values; all these aggregated values will be identical
#plot; note that there is manipulation of negatives for the directionality and absolute value
mdl_overall %>%  
  ggplot(aes(x=reorder_within(str_remove(.config, 'Preprocessor1_'), -agg_perf, agg_perf_type),
             y=abs(agg_perf),
             width=0.5)) +
  geom_col(aes(fill=(mtry))) +
  geom_label(aes(label=round(abs(agg_perf),3)), label.r=unit(0.0, "lines"), label.size=0, size=3) + 
  facet_wrap(~agg_perf_type, ncol=2, scales='free') +
  scale_x_reordered() + 
  coord_flip() +
  labs(title='General model performance over all metrics by mean overall rank',
       subtitle='Bar appearance shows parameters (width=min_n, color=mtry)',
       y='Mean over all metrics',
       x='Model name')
```
Here, we can see that the color represent mtry while the width represents min_n. Overall, the models with the thinner bars or lower width perform better than the models with the thicker bars, or greater min_n. There also does not appear to be much correlation between performance and color or mtry.

## Selecting the best model
With this information in mind as well as more help from tidymodels, we can then select the "best" model.  One way to do this is to simply choose according to some metric.  We'll decide to use `pr_auc` here just because our training data is so imbalanced.

```{r get best hyperparameters from resampling}
eval_metric <- 'pr_auc'
#show best parameters in terms of pr_auc
xgb_tune %>% show_best(eval_metric)
```

We find here that this is exactly in line with our previous assessment of overall model performance. 

```{r select best parameters}
#select best parameters
best_xgb_params <- xgb_tune %>%
  select_best(eval_metric)
#show selected parameters
best_xgb_params
```
 We can see that Model 20 (best in overall rank and mean metric performance) predictably had the highest `pr_auc`.  

# Training fit
Having identified the best hyperparameters, we can create the final fit on all of the training data:

```{r fit workflow on training data}
#finalize workflow with model hyperparameters
xgb_final_wf <- xgb_workflow %>%
  finalize_workflow(best_xgb_params)
xgb_final_wf
#using final workflow, fit on training data
xgb_final_fit <- xgb_final_wf %>%
  fit(data = train_data)
```

# Selected Model Performance Evaluation
## Cross validation metrics from best model
Let's first evaluate the performance using the cross-validation metrics from before.  However, here, we'll only look at the best model.
```{r best model cross validation}
#get best xgb metrics
best_xgb_fold_metrics <- xgb_fold_metrics %>%
  filter(.config==best_xgb_params$.config[[1]])
#plot
best_xgb_fold_metrics %>%
  mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
  ggplot(aes(x=.metric, y=.estimate, fill=.metric)) +
  geom_boxplot(outlier.shape = NA, na.rm=TRUE) +
  geom_jitter(aes(x=.metric, y=.estimate), na.rm=TRUE) +
  facet_grid(cols=vars(facet_val), scales='free') + #just to get on separate plots
  labs(title='Distribution of cross validation metrics for best hyperparameter set',
       subtitle='By metric',
       x='metric',
       y='metric estimate') +
  theme(legend.position = "none")
```
Here we can see the overall performance of the best model during its cross validation phase.  This performance mirrors the behavior of the previous cross-validation metrics.  The ROC AUC is again looking very stable, while the sens leaves much to be desired. Again, implementing a tailored thresholding strategy here will allow for better use of the model.

## Performance on training data as a whole
Here, we look at the confusion matrix for the entire training set as well as computations from the confusion matrix.
```{r extract and visualize training performance}
#get prediction class and probabilities
hp_training_preds <- 
  predict(xgb_final_fit, train_data) %>%
  bind_cols(predict(xgb_final_fit, train_data, type = "prob")) %>% 
  bind_cols(train_data %>% 
              select(particle_class))
#calculate confusion matrix
train_conf <- hp_training_preds %>%
  conf_mat(particle_class, .pred_class) 
#get summary info
t1 <- train_conf %>%
  summary() %>%
  select(-.estimator) %>%
  gridExtra::tableGrob(rows=NULL, theme=gridExtra::ttheme_default(base_size=10))
#plot cmat info
cm <- train_conf %>%
  autoplot(type='heatmap') +
  labs(title='Confusion matrix for training data')
gridExtra::grid.arrange(cm, t1, ncol=2)
```
Here, we see that 424 particles were classified as true soil samples. The accuracy estimates are at 97.1% and precision at 86.4%.

# Explaining the model
## Variable importance
What parameters are contributing most strongly to the classification?  Do we see evidence of data snooping?  Let's take a look!

```{r xgb variable importance, fig.height=6}
xgb_vip <- xgb_final_fit %>%
  pull_workflow_fit() %>%
  vi_model() %>%
  mutate(scaled_imp = Importance/sum(Importance))

xgb_vip %>%
  ggplot(aes(x=fct_reorder(Variable, scaled_imp), y=scaled_imp, fill=scaled_imp)) +
  geom_col() +
  coord_flip() +
  labs(title='Scaled coefficient magnitudes of glmnet',
       subtitle=str_c('mtry:', format(pull_workflow_fit(xgb_final_fit)$spec$args$mtry,
                                         digits=5, nsmall=2, scientific=TRUE),
                      'min_n:', format(pull_workflow_fit(xgb_final_fit)$spec$args$min_n[[2]],
                                         digits=5, nsmall=3),
                      sep=' '),
       y='scaled absolute importance',
       x='variable')
```
Here, we can see that transparency is weighted as drastically important compared to all of the other factors followed by solidity and f_length. All of the other factors aside from these 3 are weighted as fairly similar in importance.

# Save markdown file
Lastly, we'll just make sure to save this markdown file into the repo so that it may be easily accessed and viewed by everyone.  To successfully use this, **_make sure you have saved your notebook and the .nb.html has been regenerated!!_**
```{r save markdown}
fs::file_copy('43-xgboost-modeling.nb.html', './html_results/43-xgboost-modeling.nb.html', overwrite=TRUE)
```

```{r save xgboost model to box}
#move_model_info('xgb', 'save')
```


