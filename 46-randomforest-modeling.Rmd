---
title: "46-randomforest-modeling"
output:
  html_notebook:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: true
---

In this notebook, we use random forest to perform modeling.  Our general approach will be to use hyperparameter tuning via cross-validation to identify the best performing hyperparameters,  In another notebook, we will investigate the performance of the model.

### Useful packages
```{r rf specific modeling packages, results='hide'}
#load previous notebook data
source(knitr::purl("40-modeling.Rmd"))
fs::file_delete("40-modeling.R")

pacman::p_load(glmnet, tictoc, vip, tidytext, ranger)
```

# RF tidymodels specifications
The following code was used to generate the codebase that you'll find in the chunk that follows.  Some additional steps were included for simplicity in processing during later steps.
```{r generate rf recipe and general info}
#use_glmnet(particle_class ~ ., data=train_data)
```

Here, we define the specs for the feature engineering, the model, the generalized workflow, and the parameters that we'll tune using parameters selected from a max entropy grid.  These were generally obtained from the `use_*` functions of the `usemodels` package shown above.
```{r tidymodel specs}
rf_recipe <- 
  recipe(formula = particle_class ~ ., data = train_data) %>% 
  update_role(id, img_id, starts_with('filter'), hash, new_role='no_model') %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors(), -all_nominal()) 

rf_spec <- 
  rand_forest(mode = "classification", mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = 'permutation') 

   
rf_workflow <- 
  workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(rf_recipe) 

rf_parameters <- parameters(rf_spec) %>%
  update(mtry=mtry(c(1,ncol(train_data))))
rf_grid <- grid_max_entropy(rf_parameters, size=20)
```


# Hyperparameter tuning (model selection) via cross-validation
```{r perform hyperparameter tuning via xvalidation, results='hide'}
tic()
rf_tune <- rf_workflow %>%
  tune_grid(resamples = cv_folds,
            grid = rf_grid,
            metrics = metric_set(accuracy, roc_auc, pr_auc, sens, yardstick::spec, ppv, npv, f_meas),
            control = control_grid(verbose = TRUE, parallel_over='everything'))
toc()
```
We can see here that the hyperparameter tuning took about half a day for me.  This is unfortunate, but will likely not persist on other platforms.

## Cross-validation metric distributions
In this section, we're going to take a little bit of a look at the individualized performance of the models taking into each fold into account.  This will satisfy our academic curiosity in terms of machine learning and also provide some insight into the behaviors of the models.  We'll look more at the aggregated measures in a moment.

We'll first decompress the tuning metrics a bit to get them into a more friendly form for processing.  
```{r arrange cross validation metrics}
#extract the cross validation metrics for the glmnet by fold (i.e., unsummarized)
rf_fold_metrics <- rf_tune %>%
  select(id, .metrics, .notes) %>%
  unnest(.metrics)

head(rf_fold_metrics, 10)
```
Now, let's visualize this generalized performance over all the models, both the fantastic ones and the horrible ones.
```{r visualize fold performance distributions, fig.height=6, fig.width=12}
rf_fold_metrics %>%
  mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
  ggplot(aes(x=.metric, y=.estimate, fill=id)) +
  geom_boxplot(outlier.colour = 'red', na.rm=TRUE) +
  facet_wrap(facet='.metric', scales='free', nrow=2) + 
  labs(title='Distribution of performance by 20 model candidates',
       subtitle='By fold and metric',
       x='metric',
       y='metric distribution') +
  scale_x_discrete(labels=NULL)
```
We can make some general observations here:

1. The splits of the folds themselves seem to be OK.  There aren't any radically different performances here, so we don't need to suspect that maybe we have a bad set of splits.
2.  Both AUC measures are relatively good.  PR AUC is the area of under the precision/recall curve, so it cares a lot less about the negative samples than normal AUC.  It looks like the ROC AUC seems to indicate that the modeling is essentially VERY stable for all hyperparameters across all of the folds, and has pretty good performance.  Keep this in mind as we look at the rest of the performance (eg. sensitivity, specificity, etc.) which require a SPECIFIC threshold (which we have not tuned!) for classifying positive/negative.

Given the usage of the default 0.5 classification threshold:

3.  As expected with large class imbalances where the uninteresting class has more representation - the specificity and npv are great and close to 1!  Yay, we can almost always identify all of the soil particles, and almost all of the soil particles that we say are soil actually are!
4.  Accuracy is misleading as always so I won't comment except to say that it is misleading.
5.  We start to see some struggle in the in the model with correctly identifying the microdebitage samples.  Sensitivity generally around 0.35 for all of the folds - that means that we really struggle to differentiate these two classes.  However, the ppv is often quite high.  Together with the sensitivity, this means: "we REALLY struggle to identify the microdebitage, but when we predict that something IS microdebitage, it often is."  Given the nice AUC scores, this will likely improve with an enlightened thresholding strategy.

# Identifying the "best" model
Now, let's collect the metrics to see how the model did over all of the folds and all of the metrics in order to identify the best model from these candidates.  Note that this tabe looks similar to the prior tibble; the main difference here is that the results are aggregated over the folds (hence the `mean` and `n` columns).

```{r aggregating hyperparameter tuning/cross validation metrics}
tune_metrics <- rf_tune %>% 
  collect_metrics()

head(tune_metrics, 10)
```

## Basic performance overview
Let's just look at the overall (fold-less) distribution of the metrics.
```{r overall performance}
tune_metrics %>%
  ggplot(aes(x=.metric, y=mean)) +
  geom_boxplot(aes(fill=.metric), outlier.shape=NA, na.rm=TRUE) +
  geom_jitter(na.rm=TRUE) +
  facet_wrap(facets='.metric', nrow=2, scales='free') +
  theme(legend.position = 'none') +
  labs(title='Distribution of mean cv performance by 20 model candidates',
       subtitle='By metric',
       x='metric',
       y='mean cv metric') +
  scale_x_discrete(labels=NULL)

```
The general observations here pretty much mirror those of the kfold-separated performance plot above.  This is expected.

## Making sense of the hyperparameters and their influence

Let's visualize this so we can make some sort of sense out of it.
```{r fig.height=6}
tune_metrics %>%
  mutate(.metric=fct_relevel(.metric, 'roc_auc', 'pr_auc', 'f_meas', 'accuracy', 'sens', 'spec')) %>%
  ggplot(aes(x=mtry, y=min_n)) +
  geom_point(aes(fill=mean), shape=22, size=6) + 
  scale_x_log10(guide=guide_axis(angle=45)) + 
  facet_wrap(ncol=4, facet='.metric') + 
  scale_fill_gradient2(low='red', mid='yellow', high='green', midpoint=0.5) + 
  labs(title='Mean performance of min_n/mtry hyperparameter combinations',
       subtitle='By performance metric',
         x='mtry',
         y='min_n',
         fill='mean cv value')
```
Here is a difficult-to-understand plot.  The objective of this visualization is to begin to digest the relationship between the two hyperparameters and the performance given a certain metric.  Recall that hyperparameter tuning evaluates all combinations of hyperparameters.  These combos are shown as a square on a particular "subplot" of a metric of interest.  Then, there are 20 squares since there are 20 models.  And, the arrangement of all the "intersection" squares is identical.

What is of interest here is the color of the squares.  Red indicates that the performance is poor, and green indicates that the performance is great.  mtry and min_n values have little effect on performance; only in cases of f_meas and sens does an increase in mtry lead to a slight increase in performance 

## Looking at all of the metrics together to select a model
One possible way of evaluating a good model might be to rank the model according to its performance across all of the metrics.  This allows us to get a bit away from the values themselves.  However, we can also look at the values themselves and investigate the relationship.
```{r overall performance assessment, fig.height=6}
#calculate mean metrics and rank
mdl_overall <- tune_metrics %>%
  group_by(.metric) %>%
  mutate(metr_rank=rank(-mean, ties.method='average')) %>% #-mean so that rank increases (so, worse) with decreasing metric
  group_by(.config, .add=FALSE) %>%
  mutate(mean_rank = mean(metr_rank)) %>% #add mean rank
  mutate(mean_value = -mean(mean, na.rm=TRUE)) %>% #add mean value
  pivot_longer(cols=c(mean_rank, mean_value), names_to = 'agg_perf_type', values_to='agg_perf') %>%
  group_by(agg_perf_type) %>%
  filter(.metric=='pr_auc') #just pick one set of values; all these aggregated values will be identical

#plot; note that there is manipulation of negatives for the directionality and absolute value
mdl_overall %>%  
  ggplot(aes(x=reorder_within(str_remove(.config, 'Preprocessor1_'), -agg_perf, agg_perf_type),
             y=abs(agg_perf),
             width=.5)) +
  geom_col(aes(fill=mtry)) +
  geom_label(aes(label=round(abs(agg_perf),3)), label.r=unit(0.0, "lines"), label.size=0, size=3) + 
  facet_wrap(~agg_perf_type, ncol=2, scales='free') +
  scale_x_reordered() + 
  coord_flip() +
  labs(title='General model performance over all metrics by mean overall rank',
       subtitle='Bar appearance shows parameters (width=min_n, color=mtry)',
       y='Mean over all metrics',
       x='Model name')
```
Model20 here appears to be the best; mean_value numbers are highly similar though. Also, mtry value relationship with performance is uncertain, though it appears the worst performing models on these charts also have low mtry values. 

## Selecting the best model
With this information in mind as well as more help from tidymodels, we can then select the "best" model.  One way to do this is to simply choose according to some metric.  We'll decide to use `pr_auc` here just because our training data is so imbalanced.

```{r get best hyperparameters from resampling}
eval_metric <- 'pr_auc'

#show best parameters in terms of pr_auc
rf_tune %>% show_best(eval_metric)
```

We find here that this is exactly in line with our previous assessment of overall model performance. 

```{r select best parameters}
#select best parameters
best_rf_params <- rf_tune %>%
  select_best(eval_metric)

#show selected parameters
best_rf_params
```
 We can see that Model 20 (best in overall rank and mean metric performance) predictably had the highest `pr_auc`.  We also can see that this is essentially a LASSO model based on the mixture, although the penalty is small yet not unexpected.  We will use these defined parameters to fit our model.

# Training fit
Having identified the best hyperparameters, we can create the final fit on all of the training data:

```{r fit workflow on training data}
#finalize workflow with model hyperparameters
rf_final_wf <- rf_workflow %>%
  finalize_workflow(best_rf_params)
rf_final_wf

#using final workflow, fit on training data
rf_final_fit <- rf_final_wf %>%
  fit(data = train_data)
```

# Save markdown file
Lastly, we'll just make sure to save this markdown file into the repo so that it may be easily accessed and viewed by everyone.  To successfully use this, **_make sure you have saved your notebook and the .nb.html has been regenerated!!_**
```{r save markdown}
fs::file_copy('46-randomforest-modeling.nb.html', './html_results/46-randomforest-modeling.nb.html', overwrite=TRUE)
```

```{r save random forest model to box}
#move_model_info('rf', 'save')
```

