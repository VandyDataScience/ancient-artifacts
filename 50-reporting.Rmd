---
title: "50_reporting"
output: html_document
---

This file contains general functions to be applied across several model types for interpretation.

```{r required imports}
pacman::p_load(yardstick, precrec)
```

# Prediction with defined outcomes
```{r predict using fit on selected data}
get_prediction_dataframes <- function(final_fit, pred_data) {
  
  #get prediction class and probabilities
  class_pred_df <- 
    predict(final_fit, pred_data) %>%
    bind_cols(predict(final_fit, pred_data, type = "prob")) %>% 
    bind_cols(pred_data %>% 
                select(particle_class))
  
  return (class_pred_df)
}
```

# Performance metric analysis

```{r calculate and print best xval model metrics}
calculate_best_performance_metrics <- function (fold_metrics, best_params) {
  
  #get metrics
  best_fold_metrics <- fold_metrics %>%
  filter(.config==best_params$.config[[1]])

  #plot
  print(best_fold_metrics %>%
    mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
    ggplot(aes(x=.metric, y=.estimate, fill=.metric)) +
    geom_boxplot(outlier.shape = NA, na.rm=TRUE) +
    geom_jitter(aes(x=.metric, y=.estimate), na.rm=TRUE) +
    facet_grid(cols=vars(facet_val), scales='free') + #just to get on separate plots
    labs(title='Distribution of cross validation metrics for best hyperparameter set',
         subtitle='By metric',
         x='metric',
         y='metric estimate') +
    theme(legend.position = "none"))

  return (best_fold_metrics)
}
```

```{r generate confusion matrix stats}
calculate_confusion_matrix <-function (pred_frame) {
#calculate confusion matrix
pred_conf <- pred_frame %>%
  conf_mat(particle_class, .pred_class) 

#get summary info
t1 <- pred_conf %>%
  summary() %>%
  select(-.estimator) %>%
  gridExtra::tableGrob(rows=NULL, theme=gridExtra::ttheme_default(base_size=10))

#plot cmat info
cm <- pred_conf %>%
  autoplot(type='heatmap') +
  labs(title='Confusion matrix for training data')

gridExtra::grid.arrange(cm, t1, ncol=2)

return (t1)
}
```

# Plot prediction data frame
```{r plot the probabilities for predicitons}
plot_prediction_probabilities <- function(pred_frame){
  new_training_preds <- filter(pred_frame, .pred_site < 0.8)
  alpha <- ifelse(pred_frame$particle_class == "exp", 0.4, 0.1)


ggplot(data = pred_frame, mapping = aes(x = .pred_class , y =.pred_site, color = particle_class)  ) +
  geom_jitter(width = 0.4, size = 1, alpha = alpha ) +
  xlab("Model Predicted Class") + ylab("Probability of Site") +
  labs(title="Probability Prediction Plots")
  
}
```

# Calibration curve
```{r plot a calibration curve based on prediction}

plot_calibration_curve <- function(pred_frame){
  new_training_preds <- as.data.frame(pred_frame)
  new_training_preds$particle_class <- as.numeric(new_training_preds$particle_class)
  new_training_preds[, 4][new_training_preds[, 4] == 2] <-0

  #new_training_preds$particle_class <- new_training_preds$particle_class - 1
  
  
ggplot(data = new_training_preds, mapping = aes(x = .pred_exp , y = particle_class)  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +
  geom_smooth(aes(x = .pred_exp, y = particle_class ), color = "red", se = F, method = "loess") + 

  geom_abline()+
  xlab("Model Probability of Experimental") + ylab("True Particle Class (Site [0] to Experimental[1]") +
  labs(title="Calibration Curve")
  
  
}
```

# Probability-based curves
```{r function for plotting ordered probability and class relation}
plot_label_by_score <- function(preds_df){
  #Function plots ordered probability with actual class as coloring for visual inspection of misclassification
  #preds_df: prediction dataframe with minimally .pred_exp and particle_class columns
  
  preds_df %>%
    arrange(.pred_exp)%>%
    mutate(prob_order = factor(1:nrow(.)))%>%
    ggplot(aes(x=1:nrow(preds_df), y=.pred_exp, color=particle_class))+
    geom_point(size=1, alpha=0.6) +
    labs(x='Sorted order of scores',
         y='Scores',
         title='Actual class label based on increasing score')
}
```

```{r function for viewing misclassification by score}
plot_misclassification_rates <- function(preds_df){
  #Function plots misclassification rate by score bin
  #preds_df: prediction dataframe with minimally .pred_exp and particle_class columns
  
  preds_df %>%
    mutate(is_wrong = (.pred_class != particle_class)) %>%
    mutate(bin_start = str_match(cut_interval(.$.pred_exp, n=20), "[\\(\\[]([\\d\\.]+).+]")[,2]) %>%
    mutate(is_fp = ifelse(bin_start<0.5, "fn", "fp")) %>%
    group_by(bin_start) %>%
    summarise(bin_counts=sum(is_wrong)/n(), error_type=unique(is_fp)) %>%
    ggplot(aes(x=bin_start, y=bin_counts, fill=error_type)) +
    geom_col() +
    labs(x='score interval', y='Percentage misclassified in bin', title='Misclassification rate in probability interval') + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```


```{r function for ROC curves}
plot_performance_curves <- function(pred_dfs, model_names=NULL, pos_class='exp'){
  #Function plots ROC and PR AUC curves for one or more models and returns the eval information
  #preds_df: a single dataframe or list of dataframes containing prediction info (.pred_exp and particle_class at least)
  #model_names: VECTOR of model names corresponding to order of preds_df
  #pos_class (default 'exp'): positive class or class of interest
  
  #if passed in a single dataframe for plotting, make into list
  if('tbl_df' %in% class(pred_dfs))
    pred_dfs=list(pred_dfs)
  
  #get list of scores for plotting
  scores_list <- pred_dfs %>%
    map(~select(., .pred_exp)) %>%
    join_scores()
  
  #get list of labels for plotting
  labels_list <- pred_dfs %>%
    map(~select(., particle_class)) %>%
    join_labels()
  
  #fix model names if necessary
  if(is.null(model_names)){
    model_names = str_c('Model', 1:length(pred_dfs))
  }
  
  #calculate model eval and autoplot
  model_eval = evalmod(scores = scores_list, labels = labels_list, modnames = model_names, posclass = pos_class)
  autoplot(model_eval)
  return(model_eval)
}
```