---
title: "50_reporting"
output: html_notebook
---

Report the results of the project with relevant performance and modeling considerations.

# Prediction with defined outcomes
```{r predict using fit on selected data}
get_prediction_dataframes <- function(final_fit, pred_data) {
  
  #get prediction class and probabilities
  class_pred_df <- 
    predict(final_fit, pred_data) %>%
    bind_cols(predict(final_fit, pred_data, type = "prob")) %>% 
    bind_cols(pred_data %>% 
                select(particle_class))
  
  return (class_pred_df)
}
```

# Performance metric analysis

```{r calculate and print best xval model metrics}
calculate_best_performance_metrics <- function (fold_metrics, best_params) {
  
  #get metrics
  best_fold_metrics <- fold_metrics %>%
  filter(.config==best_params$.config[[1]])

  #plot
  best_fold_metrics %>%
    mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
    ggplot(aes(x=.metric, y=.estimate, fill=.metric)) +
    geom_boxplot(outlier.shape = NA, na.rm=TRUE) +
    geom_jitter(aes(x=.metric, y=.estimate), na.rm=TRUE) +
    facet_grid(cols=vars(facet_val), scales='free') + #just to get on separate plots
    labs(title='Distribution of cross validation metrics for best hyperparameter set',
         subtitle='By metric',
         x='metric',
         y='metric estimate') +
    theme(legend.position = "none")

  return (best_fold_metrics)
}
```

```{r generate confusion matrix stats}
calculate_confusion_matrix <-function (pred_frame) {
#calculate confusion matrix
pred_conf <- pred_frame %>%
  conf_mat(particle_class, .pred_class) 

#get summary info
t1 <- pred_conf %>%
  summary() %>%
  select(-.estimator) %>%
  gridExtra::tableGrob(rows=NULL, theme=gridExtra::ttheme_default(base_size=10))

#plot cmat info
cm <- pred_conf %>%
  autoplot(type='heatmap') +
  labs(title='Confusion matrix for training data')

gridExtra::grid.arrange(cm, t1, ncol=2)

return (t1)
}
```

# Plot prediction data frame
```{r plot the probabilities for predicitons}
plot_prediction_probabilities <- function(pred_frame){
  new_training_preds <- filter(pred_frame, .pred_site < 0.8)
  alpha <- ifelse(pred_frame$particle_class == "exp", 0.4, 0.1)


ggplot(data = pred_frame, mapping = aes(x = .pred_class , y =.pred_site, color = particle_class)  ) +
  geom_jitter(width = 0.4, size = 1, alpha = alpha ) +
  xlab("Model Predicted Class") + ylab("Probability of Site") +
  labs(title="Probability Prediction Plots")
  
}
```

# Calibration curve
```{r plot a calibration curve based on prediction}

plot_calibration_curve <- function(pred_frame){
  new_training_preds <- as.data.frame(pred_frame)
  new_training_preds$particle_class <- as.numeric(new_training_preds$particle_class)
  new_training_preds[, 4][new_training_preds[, 4] == 2] <-0

  #new_training_preds$particle_class <- new_training_preds$particle_class - 1
  
  
ggplot(data = new_training_preds, mapping = aes(x = .pred_exp , y = particle_class)  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +
  geom_smooth(aes(x = .pred_exp, y = particle_class ), color = "red", se = F, method = "loess") + 

  geom_abline()+
  xlab("Model Probability of Experimental") + ylab("True Particle Class (Site [0] to Experimental[1]") +
  labs(title="Calibration Curve")
  
  
}
```