---
title: "50_reporting"
output: html_notebook
---

Report the results of the project with relevant performance and modeling considerations.

# Prediction with defined outcomes
```{r predict using fit on selected data}
get_prediction_dataframes <- function(final_fit, pred_data) {
  
  #get prediction class and probabilities
  class_pred_df <- 
    predict(final_fit, pred_data) %>%
    bind_cols(predict(final_fit, pred_data, type = "prob")) %>% 
    bind_cols(pred_data %>% 
                select(particle_class))
  
  return (class_pred_df)
}
```

# Performance metric analysis

```{r calculate and print best xval model metrics}
calculate_best_performance_metrics <- function (fold_metrics, best_params) {
  
  #get metrics
  best_fold_metrics <- fold_metrics %>%
  filter(.config==best_params$.config[[1]])

  #plot
  best_fold_metrics %>%
    mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
    ggplot(aes(x=.metric, y=.estimate, fill=.metric)) +
    geom_boxplot(outlier.shape = NA, na.rm=TRUE) +
    geom_jitter(aes(x=.metric, y=.estimate), na.rm=TRUE) +
    facet_grid(cols=vars(facet_val), scales='free') + #just to get on separate plots
    labs(title='Distribution of cross validation metrics for best hyperparameter set',
         subtitle='By metric',
         x='metric',
         y='metric estimate') +
    theme(legend.position = "none")

  return (best_fold_metrics)
}
```

```{r generate confusion matrix stats}
calculate_confusion_matrix <-function (pred_frame) {
#calculate confusion matrix
pred_conf <- pred_frame %>%
  conf_mat(particle_class, .pred_class) 

#get summary info
t1 <- pred_conf %>%
  summary() %>%
  select(-.estimator) %>%
  gridExtra::tableGrob(rows=NULL, theme=gridExtra::ttheme_default(base_size=10))

#plot cmat info
cm <- pred_conf %>%
  autoplot(type='heatmap') +
  labs(title='Confusion matrix for training data')

gridExtra::grid.arrange(cm, t1, ncol=2)

return (pred_conf)
}
```

```{r function for ROC curves}
plot_performance_curves <- function(pred_dfs, model_names=NULL, pos_class='exp'){
  #Function plots ROC and PR AUC curves for one or more models and returns the eval information
  #preds_df: a single dataframe or list of dataframes containing prediction info (.pred_exp and particle_class at least)
  #model_names: VECTOR of model names corresponding to order of preds_df
  #pos_class (default 'exp'): positive class or class of interest
  
  #if passed in a single dataframe for plotting, make into list
  if('tbl_df' %in% class(pred_dfs))
    pred_dfs=list(pred_dfs)
  
  #get list of scores for plotting
  scores_list <- pred_dfs %>%
    map(~select(., .pred_exp)) %>%
    join_scores()
  
  #get list of labels for plotting
  labels_list <- pred_dfs %>%
    map(~select(., particle_class)) %>%
    join_labels()
  
  #fix model names if necessary
  if(is.null(model_names)){
    model_names = str_c('Model', 1:length(pred_dfs))
  }
  
  #calculate model eval and autoplot
  model_eval = evalmod(scores = scores_list, labels = labels_list, modnames = model_names, posclass = pos_class)
  autoplot(model_eval)
  return(model_eval)
}
```