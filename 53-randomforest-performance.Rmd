---
title: "53-randomforest-performance"
output:
  html_notebook:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: true
---

In this notebook, we will look at the peformance of the randomForest modeling strategy.

**Note that prior to running this notebook, 10, 40, and 46-randomForest-modeling must already have been run.**


### Useful packages
```{r load packages, results='hide'}
#load previous notebook data
source(knitr::purl("50-reporting.Rmd"))
fs::file_delete("50-reporting.R")

pacman::p_load(glmnet, tictoc, vip, tidytext, ranger)
```

# Selected Model Performance Evaluation
## Cross validation metrics from best model
Let's first evaluate the performance using the cross-validation metrics from before.  However, here, we'll only look at the best model.
```{r best model cross validation}
#get best glmnet metrics
best_rf_fold_metrics <- calculate_best_performance_metrics(rf_fold_metrics, best_rf_params)
```
Here we can see the overall performance of the best model during its cross validation phase.  This performance mirrors the behavior of the previous cross-validation metrics.  The ROC AUC is again looking very stable, while the PR AUC leaves a bit to be desired.  The metrics which rely on threshold are very stable and performant for calculations which rely or focus heavily on the class of disinterest.  On the other hand, the sensitivity and ppv leave much to be desired.  Again, implementing a tailored thresholding strategy here will allow for better use of the model.

## Performance on training data as a whole
Here, we look at the confusion matrix for the entire training set as well as computations from the confusion matrix.
```{r extract and visualize training performance}
#get prediction class and probabilities
hp_training_preds <- get_prediction_dataframes(rf_final_fit, train_data)

#calculate confusion matrix
rf_conf_mat <- calculate_confusion_matrix(hp_training_preds)
```
These results allow us several important insights:
1. The confusion matrix reflects the distribution of the data
2. The calculated metrics correctly reflect the target class formulation
3. The performance leaves room for improvement in terms of metrics calculated based on a threshold

# Explaining the model
## Variable imporance
What parameters are contributing most strongly to the classification?  Do we see evidence of data snooping?  Let's take a look!

```{r glmnet variable importance, fig.height=6}
rf_vip <- rf_final_fit %>%
  pull_workflow_fit() %>%
  vi_model() %>%
  mutate(scaled_imp = Importance/sum(Importance)) 

rf_vip %>%
  ggplot(aes(x=fct_reorder(Variable, scaled_imp), y=scaled_imp, fill=scaled_imp)) +
  geom_col() +
  coord_flip() +
  labs(title='Variable Importance in Random Forest',
       subtitle=str_c('Mtry:', format(pull_workflow_fit(rf_final_fit)$spec$args$mtry,
                                         digits=5, nsmall=2, scientific=TRUE),
                      'Min_n:', format(pull_workflow_fit(rf_final_fit)$spec$args$min_n[[2]],
                                         digits=5, nsmall=3),
                      sep=' '),
       y='scaled absolute importance',
       x='variable')
```
Transparency holds the greatest importance for these models and for determining microdebitage, followed by fiber_length, f_length, and e_length. As discussed in the coding meeting, these last three variables should be investigated to see if one of these length variables holds the most importance (to the extent that the other two variables derive their importance only in their relationship to this dominant length variable). 

# Save markdown file
Lastly, we'll just make sure to save this markdown file into the repo so that it may be easily accessed and viewed by everyone.  To successfully use this, **_make sure you have saved your notebook and the .nb.html has been regenerated!!_**
```{r save markdown}
fs::file_copy('53-randomforest-performance.nb.html', './html_results/53-randomforest-performance.nb.html', overwrite=TRUE)
```

