---
title: "55-cv-comparison"
output:
  html_notebook:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: true
---

In this notebook, we compare the effectiveness of the various model types based on the performance of cross validation metrics. 

### Useful packages
```{r load required packages, results='hide'}
#load previous notebook data
source(knitr::purl("40-modeling.Rmd"))
source(knitr::purl("50-reporting.Rmd"))
fs::file_delete("40-modeling.R")
fs::file_delete("50-reporting.R")

pacman::p_load(vip)
```

# Load data (if desired)
```{r load all models}
mdl_list <- list('glmnet', 'nb', 'rf', 'xgb')
map(mdl_list, ~move_model_info(., 'load', box_dir = path.expand('~/../Box/DSI_AncientArtifacts/')))
```

# Aggregate performance metrics
```{r aggregating hyperparameter tuning/cross validation metrics}
best_rf_fold_metrics <- calculate_best_performance_metrics(rf_fold_metrics, best_rf_params) %>%
  select(id, .metric, .estimate, .config) %>%
  mutate(modeltype = "rf")

best_nb_fold_metrics <- calculate_best_performance_metrics(nb_fold_metrics, best_nb_params) %>%
  select(id, .metric, .estimate, .config) %>%
  mutate(modeltype = "nb")

best_xgb_fold_metrics <- calculate_best_performance_metrics(xgb_fold_metrics, best_xgb_params) %>%
  select(id, .metric, .estimate, .config) %>%
  mutate(modeltype = "xgb")

best_glmnet_fold_metrics <- calculate_best_performance_metrics(glmnet_fold_metrics, best_glmnet_params) %>% 
  select(id, .metric, .estimate, .config) %>%
  mutate(modeltype = "glmnet")

best_fold_metrics <- bind_rows(best_rf_fold_metrics, best_nb_fold_metrics, best_xgb_fold_metrics, best_glmnet_fold_metrics) 

head(best_fold_metrics, 10)
```

## Basic performance overview
Let's just look at the overall distribution of the metrics.
```{r overall performance}
best_fold_metrics %>%
  mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
  ggplot(aes(x=.metric, y=.estimate, fill=modeltype)) +
  geom_boxplot(outlier.colour = 'red', na.rm=TRUE) +
  facet_wrap(facet='.metric', scales='free', nrow=2) + 
  labs(title='Comparison of performance of CV metrics between 4 model types',
       subtitle='By fold and metric',
       x='metric',
       y='metric distribution') +
  scale_x_discrete(labels=NULL)

```


```{r performance summaries, message=FALSE}
best_fold_metrics %>%
  group_by(modeltype, .metric) %>%
  summarise(mean_value = mean(.estimate, na.rm = TRUE)) %>%
  ggplot(mapping = aes(x=.metric, y=mean_value, fill = modeltype))+
    geom_col(position = "dodge")
  
```

# Compare ROC curves
```{r compare rocs}
#get list of prediction dataframes
model_preds_dfs <- mdl_list %>%
  map(~get(str_c(., '_final_fit'))) %>%
  map(~get_prediction_dataframes(., train_data))

#give list elements names
names(model_preds_dfs) <- mdl_list

#plot performance
plot_performance_curves(model_preds_dfs, unlist(mdl_list))
```

# Calibration Plots
```{r compare calibrations}

#add model name
cal_df <- model_preds_dfs %>%
  map2_df(names(model_preds_dfs), function(df, mdl_name){mutate(df, mdl_name=mdl_name)})

#plot calibration curves
plot_calibration_curve(cal_df)

```


# Save markdown file
Lastly, we'll just make sure to save this markdown file into the repo so that it may be easily accessed and viewed by everyone.  To successfully use this, **_make sure you have saved your notebook and the .nb.html has been regenerated!!_**
```{r save markdown}
#fs::file_copy('55-cv-comparison.nb.html', './html_results/55-cv-comparison.nb.html', overwrite=TRUE)
```

