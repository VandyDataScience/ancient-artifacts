---
title: "55-cv-comparison"
output:
  html_notebook:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: true
---

In this notebook, we compare the effectiveness of the various model types based on the performance of cross validation metrics. 

### Useful packages
```{r load required packages, results='hide'}
#load previous notebook data
source(knitr::purl("40-modeling.Rmd"))
source(knitr::purl("50-reporting.Rmd"))
fs::file_delete("40-modeling.R")
fs::file_delete("50-reporting.R")

pacman::p_load(vip)
```

# Load data (if desired)
```{r load all models}
mdl_list <- list('glmnet', 'nb', 'rf', 'xgb')
map(mdl_list, ~move_model_info(., 'load', box_dir = path.expand('~/../Box/DSI_AncientArtifacts/')))
```

# Aggregate performance metrics
```{r aggregating hyperparameter tuning/cross validation metrics}
#get best performance metrics for all models
best_fold_metrics <- mdl_list %>%
  map(~calculate_best_performance_metrics(get(str_c(., '_fold_metrics')), 
                                          get(str_c('best_', ., '_params')), .))
#name each element in list
names(best_fold_metrics) <- mdl_list

#aggregate all fold metrics and mutate all names
best_fold_metrics <- best_fold_metrics %>%
  map(~select(., id, .metric, .estimate, .config)) %>%
  map2_dfr(names(best_fold_metrics), function(df, mdl_name){mutate(df, modeltype=mdl_name)})
```

## Basic performance overview
Let's just look at the overall distribution of the metrics.
```{r overall performance}
best_fold_metrics %>%
  mutate(facet_val = if_else(.metric== 'roc_auc' | .metric=='pr_auc' | .metric=='f_meas', 'Aggregate metrics', 'Confusion matrix metrics')) %>%
  ggplot(aes(x=.metric, y=.estimate, fill=modeltype)) +
  geom_boxplot(outlier.colour = 'red', na.rm=TRUE) +
  facet_wrap(facet='.metric', scales='free', nrow=2) + 
  labs(title='Comparison of performance of CV metrics between 4 model types',
       subtitle='By fold and metric',
       x='metric',
       y='metric distribution') +
  scale_x_discrete(labels=NULL)

```


```{r performance summaries, message=FALSE}
best_fold_metrics %>%
  group_by(modeltype, .metric) %>%
  summarise(mean_value = mean(.estimate, na.rm = TRUE)) %>%
  ggplot(mapping = aes(x=.metric, y=mean_value, fill = modeltype))+
    geom_col(position = "dodge")
  
```

# Compare ROC curves
```{r compare rocs}
#get list of prediction dataframes
model_preds_dfs <- mdl_list %>%
  map(~get(str_c(., '_final_fit'))) %>%
  map(~get_prediction_dataframes(., train_data))

#give list elements names
names(model_preds_dfs) <- mdl_list

#plot performance
plot_performance_curves(model_preds_dfs, unlist(mdl_list))
```

# Calibration Plots
```{r compare calibrations}

#add model name
cal_df <- model_preds_dfs %>%
  map2_df(names(model_preds_dfs), function(df, mdl_name){mutate(df, mdl_name=mdl_name)})

#plot calibration curves
plot_calibration_curve(cal_df)

```


# Save markdown file
```{r save markdown}
#fs::file_copy('55-cv-comparison.nb.html', './html_results/55-cv-comparison.nb.html', overwrite=TRUE)
```

